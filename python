import random
import logging
from typing import List, Dict
from src.llm_pool.llm_pool import LLMPool
from src.moderator.moderator import Moderator
from src.heuristics.principles import Principles
from src.utils.metrics import calculate_response_metrics, calculate_sentiment_scores

class ABTestRunner:
    def __init__(self, config: Dict):
        self.config = config
        self.openai_llm_pool = LLMPool(config, api_type='openai')
        self.claude_llm_pool = LLMPool(config, api_type='anthropic')
        self.principles = Principles(config['principles']['version_control_file'])
        self.openai_moderator = Moderator(self.openai_llm_pool, self.principles)
        self.claude_moderator = Moderator(self.claude_llm_pool, self.principles)
        self.logger = logging.getLogger(__name__)

    def run_ab_test(self, input_text: str, num_iterations: int = 5) -> Dict[str, List[Dict]]:
        results = {
            'openai': [],
            'anthropic': []
        }

        for i in range(num_iterations):
            self.logger.info(f"Starting iteration {i+1} of {num_iterations}")
            
            # Run OpenAI test
            openai_result = list(self.openai_moderator.start_discussion_stream(input_text))
            results['openai'].append(openai_result)
            self.logger.info(f"OpenAI test completed for iteration {i+1}")

            # Run Claude test
            claude_result = list(self.claude_moderator.start_discussion_stream(input_text))
            results['anthropic'].append(claude_result)
            self.logger.info(f"Claude test completed for iteration {i+1}")

        return results

    def analyze_results(self, results: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        analysis = {
            'openai': self._analyze_api_results(results['openai'], 'OpenAI'),
            'anthropic': self._analyze_api_results(results['anthropic'], 'Claude')
        }
        
        # Compare the results
        comparison = self._compare_results(analysis['openai'], analysis['anthropic'])
        analysis['comparison'] = comparison

        return analysis

    def _analyze_api_results(self, api_results: List[Dict], api_name: str) -> Dict:
        total_responses = sum(len(result) for result in api_results)
        avg_responses = total_responses / len(api_results)
        
        response_metrics = calculate_response_metrics(api_results)
        sentiment_scores = calculate_sentiment_scores(api_results)
        
        analysis = {
            'average_responses_per_discussion': avg_responses,
            'total_discussions': len(api_results),
            'response_metrics': response_metrics,
            'sentiment_scores': sentiment_scores
        }
        
        self.logger.info(f"Analysis completed for {api_name}")
        return analysis

    def _compare_results(self, openai_analysis: Dict, claude_analysis: Dict) -> Dict:
        comparison = {}
        
        for metric in openai_analysis.keys():
            if isinstance(openai_analysis[metric], (int, float)):
                difference = openai_analysis[metric] - claude_analysis[metric]
                percentage_difference = (difference / openai_analysis[metric]) * 100
                comparison[metric] = {
                    'difference': difference,
                    'percentage_difference': percentage_difference
                }
        
        self.logger.info("Comparison of OpenAI and Claude results completed")
        return comparison
